{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pablo/long-transformers\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 8\n",
    "L = 2048\n",
    "D = 128\n",
    "\n",
    "ATT_WINDOW = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.randn(B, L, D)\n",
    "K = torch.randn(B, L, D)\n",
    "V = torch.randn(B, L, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    1,    2,  ...,  510,  511,  512],\n",
      "        [   0,    1,    2,  ...,  510,  511,  512],\n",
      "        [   0, 1536, 1537,  ..., 2045, 2046, 2047],\n",
      "        [   0, 1536, 1537,  ..., 2045, 2046, 2047]])\n",
      "torch.Size([2048, 513])\n"
     ]
    }
   ],
   "source": [
    "# local attention\n",
    "# 0, 1, 2, 3\n",
    "# 0, 1, 2, 3\n",
    "# 0, 2, 3, 4\n",
    "# 0, 3, 4, 5\n",
    "# ...\n",
    "offset_start = -ATT_WINDOW // 2 + 1\n",
    "offset_end = ATT_WINDOW // 2 + 1\n",
    "offsets = torch.arange(offset_start, offset_end).view(1, -1)\n",
    "\n",
    "min_idx = 1 - offset_start\n",
    "max_idx = L - offset_end\n",
    "start_idxs = torch.arange(0, L).clip(min_idx, max_idx).view(-1, 1)\n",
    "idxs = start_idxs + offsets\n",
    "\n",
    "# add 0 to the left\n",
    "idxs = torch.cat([torch.zeros_like(idxs[:, :1]), idxs], dim=1).long()\n",
    "\n",
    "print(idxs[[0, 1, -2, -1]])\n",
    "print(idxs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2048, 513, 128])\n",
      "torch.Size([8, 2048, 513, 128])\n",
      "torch.Size([8, 2048, 128])\n",
      "torch.Size([8, 2048, 128])\n"
     ]
    }
   ],
   "source": [
    "# i-th token has to attend to tokens idxs[i]\n",
    "print(K[:, idxs, :].shape)  # [B, L, W, D]\n",
    "print(V[:, idxs, :].shape)  # [B, L, W, D]\n",
    "print(Q.shape) # [B, L, D]\n",
    "\n",
    "attention_scores = torch.einsum('b l w d , b l d -> b l w', K[:, idxs, :], Q) / (D ** 0.5)  # [B, L, W]\n",
    "attention_weights = F.softmax(attention_scores, dim=-1)  # [B, L, W]\n",
    "outputs = torch.einsum('b l w , b l w d -> b l d', attention_weights, V[:, idxs, :])  # [B, L, D]\n",
    "\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.5883e-01,  2.7065e+00, -2.9602e+00,  ...,  1.3942e+00,\n",
       "           7.2766e-01, -4.6214e-01],\n",
       "         [ 4.2545e-01,  6.6448e-01, -3.2067e-01,  ...,  1.0622e+00,\n",
       "          -9.0218e-01, -6.7893e-01],\n",
       "         [-8.4039e-01, -4.7138e-01, -1.8840e+00,  ..., -4.0091e-01,\n",
       "           2.4124e-01,  6.7348e-02],\n",
       "         ...,\n",
       "         [-7.4906e-01,  1.6485e+00,  3.8024e-01,  ..., -3.7892e-01,\n",
       "          -1.0379e+00, -6.9358e-01],\n",
       "         [-6.5863e-01, -9.9215e-01,  2.2255e-02,  ...,  1.3609e-01,\n",
       "           1.4068e+00, -1.1733e+00],\n",
       "         [-1.0103e+00, -4.9050e-01,  6.5288e-02,  ..., -6.6780e-01,\n",
       "          -6.5068e-01, -2.5657e-01]],\n",
       "\n",
       "        [[ 4.9925e-01,  1.0430e+00,  1.4871e-01,  ...,  2.0554e-01,\n",
       "          -1.1469e+00, -1.3343e+00],\n",
       "         [-1.4126e+00,  6.1874e-01,  9.4093e-01,  ..., -8.7320e-02,\n",
       "          -1.7498e+00,  3.6725e-01],\n",
       "         [-1.6615e+00,  1.8562e-01, -4.5033e-01,  ..., -4.8201e-01,\n",
       "           3.0066e-01, -8.1920e-02],\n",
       "         ...,\n",
       "         [-7.5891e-01,  2.5804e-01,  1.6214e-02,  ..., -5.0491e-01,\n",
       "          -1.8693e+00,  6.9363e-01],\n",
       "         [-2.2486e-03,  9.0985e-01, -1.5753e-01,  ..., -7.2455e-01,\n",
       "           2.3510e-01, -2.0526e-01],\n",
       "         [-1.3822e+00, -3.7758e-01, -1.1260e+00,  ..., -1.4300e-01,\n",
       "           5.3288e-01, -5.1728e-01]],\n",
       "\n",
       "        [[ 1.2574e+00,  1.5648e+00, -1.0070e+00,  ...,  9.0190e-01,\n",
       "          -1.1434e-01, -3.9640e-01],\n",
       "         [ 1.3546e-01, -2.1747e-01,  7.7242e-01,  ..., -6.0778e-01,\n",
       "          -1.3829e+00,  8.7992e-01],\n",
       "         [ 3.5771e-01, -1.6986e+00, -5.8598e-01,  ...,  3.0026e-02,\n",
       "           3.8841e-01,  5.1071e-01],\n",
       "         ...,\n",
       "         [-6.2989e-01, -2.1012e+00, -1.3713e+00,  ...,  9.0291e-01,\n",
       "          -1.2389e+00, -5.7907e-01],\n",
       "         [ 1.8564e-01,  1.6017e-01, -7.9096e-02,  ..., -1.3549e+00,\n",
       "           9.9260e-01, -6.6471e-01],\n",
       "         [ 2.6077e-02,  9.1690e-01, -1.1129e+00,  ..., -1.9442e+00,\n",
       "           7.3246e-01,  4.2322e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.1627e-01, -5.4972e-01,  1.2350e+00,  ..., -8.4386e-01,\n",
       "          -9.9059e-01, -7.7485e-01],\n",
       "         [ 1.4423e-01,  6.8571e-02,  8.3792e-01,  ...,  1.4244e+00,\n",
       "          -2.5641e-01, -1.3302e-01],\n",
       "         [ 4.9105e-01, -1.1372e+00,  9.2820e-01,  ..., -8.6955e-01,\n",
       "           8.3658e-01, -1.2020e+00],\n",
       "         ...,\n",
       "         [ 2.2631e-02, -5.4575e-01, -3.3319e-01,  ..., -1.2577e+00,\n",
       "          -6.7643e-01, -5.2134e-01],\n",
       "         [ 7.0162e-02,  7.5956e-01,  1.3978e-01,  ..., -2.4216e-01,\n",
       "          -1.2631e+00,  6.7336e-01],\n",
       "         [-5.6742e-01,  7.1026e-01,  1.9075e+00,  ...,  7.6910e-01,\n",
       "          -6.9155e-02, -1.5607e-01]],\n",
       "\n",
       "        [[ 3.2183e-01,  7.8685e-01,  1.6020e+00,  ...,  1.6068e+00,\n",
       "           4.2762e-01,  9.8420e-01],\n",
       "         [-1.1359e+00, -1.0504e+00,  4.3098e-01,  ..., -7.9005e-02,\n",
       "          -2.1347e-01, -1.8204e-01],\n",
       "         [-5.7833e-01,  1.8928e+00,  1.3579e+00,  ..., -2.3708e-01,\n",
       "          -1.5338e-01, -1.3719e+00],\n",
       "         ...,\n",
       "         [ 1.0199e+00, -6.0885e-01,  1.8152e+00,  ...,  2.8774e-01,\n",
       "          -5.8707e-01, -8.0796e-01],\n",
       "         [-1.0390e+00, -9.0116e-01, -2.1109e-01,  ..., -1.1399e-01,\n",
       "          -8.7376e-01, -6.6435e-01],\n",
       "         [ 2.4789e+00,  1.1733e+00, -4.2807e-01,  ...,  1.7920e+00,\n",
       "           6.5998e-01, -1.5501e+00]],\n",
       "\n",
       "        [[ 9.7455e-01,  2.3052e+00, -2.1284e-01,  ..., -2.7454e-01,\n",
       "          -8.2704e-01,  9.8181e-01],\n",
       "         [ 1.0979e-01,  2.8728e-01,  5.1164e-01,  ...,  1.5685e-01,\n",
       "           1.1234e+00, -1.6362e+00],\n",
       "         [-9.5158e-01,  9.9325e-02, -3.6007e-01,  ..., -1.2474e+00,\n",
       "          -5.6697e-01,  7.1436e-01],\n",
       "         ...,\n",
       "         [-2.5042e-01,  2.2460e-01,  3.4401e-01,  ...,  7.4934e-01,\n",
       "          -1.6584e-01,  3.2798e-01],\n",
       "         [-5.6474e-01,  1.3337e+00, -1.2693e+00,  ...,  8.0782e-01,\n",
       "           1.8459e+00, -6.5617e-01],\n",
       "         [-1.4165e+00, -2.5368e-01, -1.4781e+00,  ...,  1.1718e+00,\n",
       "           3.2066e-01, -9.5208e-01]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  -1,   -2,    0,  ...,  510,  511,  512],\n",
      "        [  -1,   -2,    0,  ...,  510,  511,  512],\n",
      "        [  -1,   -2,    0,  ..., 2045, 2046, 2047],\n",
      "        [  -1,   -2,    0,  ..., 2045, 2046, 2047]])\n",
      "torch.Size([2048, 515])\n"
     ]
    }
   ],
   "source": [
    "global_tokens = torch.tensor([-1, -2]).repeat(L, 1)\n",
    "idxs = torch.cat([global_tokens, idxs], dim=1)\n",
    "print(idxs[[0, 1, -2, -1]])\n",
    "print(idxs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.models.modules.local_multihead_self_attention import LocalMultiheadSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 8\n",
    "H = 8\n",
    "L = 2048\n",
    "D = 128\n",
    "\n",
    "ATT_WINDOW = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = LocalMultiheadSelfAttention(D, H, ATT_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.randn(B, L, D)\n",
    "attention_mask = torch.ones(B, L).bool()\n",
    "outputs = mha(embeddings, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2048, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying unfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.masked import masked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 8\n",
    "L = 2048\n",
    "D = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9995, -0.9990, -0.9985,  ..., -0.9985, -0.9990, -0.9995])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_rng = - torch.abs(L - 1 - torch.arange(0, 2 * L - 1)) / L\n",
    "l_rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = l_rng.unfold(0, L, 1).view(1, 1, L, L).expand(B, 1, L, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = l_rng.unfold(0, L, 1)# .flip(dims=(1,))  # [L, L]\n",
    "padding_mask = torch.rand(B, L) > 0.5  # [B, L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view both as [B, 1, L, L]\n",
    "attn_mask = attn_mask.view(1, 1, L, L).expand(B, 1, L, L)\n",
    "padding_mask = padding_mask.view(B, 1, 1, L).expand(B, 1, L, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16777216, 16384)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attn_mask.untyped_storage()), len(padding_mask.untyped_storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.micromamba/envs/long-transformers/lib/python3.10/site-packages/torch/masked/maskedtensor/core.py:156: UserWarning: The PyTorch API of MaskedTensors is in prototype stage and will change in the near future. Please open a Github issue for features requests and see our documentation on the torch.masked module for further information about the project.\n",
      "  warnings.warn((\"The PyTorch API of MaskedTensors is in prototype stage \"\n"
     ]
    }
   ],
   "source": [
    "mt = masked_tensor(attn_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134217728"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mt.untyped_storage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 8\n",
    "H = 8\n",
    "L = 2048\n",
    "D = 128\n",
    "WINDOW_SIZE = 32 * 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = torch.randn(B, H, L, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_logits = nn.Parameter(torch.Empty(H, WINDOW_SIZE, 1))\n",
    "alpha_logits = nn.Parameter(torch.Empty(H, 1, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
