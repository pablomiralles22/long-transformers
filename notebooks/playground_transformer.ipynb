{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 2048 # @param {type:\"integer\"}\n",
    "TRAIN_BATCH_SIZE = 32 # @param {type:\"integer\"}\n",
    "VALID_BATCH_SIZE = 32 # @param {type:\"integer\"}\n",
    "EPOCHS = 20 # @param {type:\"integer\"}\n",
    "LEARNING_RATE = 5e-2 # @param {type:\"number\"}\n",
    "WEIGHT_DECAY = 1e-5 # @param {type:\"number\"}\n",
    "\n",
    "MODEL_FILE_NAME = \"models/transformer.bin\"\n",
    "LOG_FILE_NAME = \"logs/transformer.log\"\n",
    "\n",
    "PAD_TOKEN = 0\n",
    "CLS_TOKEN = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Architecture params\n",
    "EMBEDDING_PARAMS = dict(\n",
    "    num_embeddings=255+2,  # PAD, CLS, chars\n",
    "    embedding_dim=512,\n",
    ")\n",
    "TRANSFORMER_PARAMS = dict(\n",
    "    nhead=8,\n",
    "    d_model=512,\n",
    "    dim_feedforward=2048,\n",
    "    batch_first=True,\n",
    ")\n",
    "NUM_TRANSFORMER_ENCODER_LAYERS = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Let's start by building a model with a few convolutional layers to pool local information and join letters into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, d_model):\n",
    "    if d_model % 2 != 0:\n",
    "        raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
    "                         \"odd dim (got dim={:d})\".format(d_model))\n",
    "    pe = torch.zeros(length, d_model)\n",
    "    position = torch.arange(0, length, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(\n",
    "        torch.arange(0, d_model, 2, dtype=torch.float) *\n",
    "        (- math.log(10000.0) / d_model)\n",
    "    )\n",
    "    pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "    pe.requires_grad = False\n",
    "\n",
    "    return pe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_params: dict,\n",
    "        transformer_encoder_params: dict,\n",
    "        num_transformer_encoder_layers: int\n",
    "    ):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(**embedding_params)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(**transformer_encoder_params)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_transformer_encoder_layers)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,  # (...BATCH, LENGTH)\n",
    "        src_key_padding_mask=None,  # (...BATCH, LENGTH)\n",
    "    ):\n",
    "        x = self.embedding(input_ids)  # (...BATCH, LENGTH, EMBED_DIM)\n",
    "        *_, length, d_model = x.shape\n",
    "        x += positional_encoding(length, d_model)\n",
    "        x = self.transformer_encoder.forward(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithHead(nn.Module):\n",
    "    def __init__(self, model, d_model):\n",
    "        super(ModelWithHead, self).__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.head = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,  # (...BATCH, LENGTH)\n",
    "        attention_mask=None,  # (...BATCH, LENGTH)\n",
    "    ):\n",
    "        x = self.model(input_ids, attention_mask)\n",
    "        x = x[:, 0, :]\n",
    "        x = self.head(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(MODEL_FILE_NAME):\n",
    "    model = torch.load(MODEL_FILE_NAME)\n",
    "else:\n",
    "    model = TransformerModel(\n",
    "        embedding_params=EMBEDDING_PARAMS,\n",
    "        transformer_encoder_params=TRANSFORMER_PARAMS,\n",
    "        num_transformer_encoder_layers=NUM_TRANSFORMER_ENCODER_LAYERS,\n",
    "    )\n",
    "model = ModelWithHead(model, d_model=TRANSFORMER_PARAMS[\"d_model\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19046401"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader and collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, root_dir, split=\"train\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the data.\n",
    "            split (string): One of \"train\" or \"test\" to specify the split.\n",
    "        \"\"\"\n",
    "        self.root_dir = os.path.join(root_dir, split)\n",
    "        pos_dir = os.path.join(self.root_dir, \"pos\")\n",
    "        self.pos_files = os.listdir(pos_dir)\n",
    "\n",
    "        neg_dir = os.path.join(self.root_dir, \"neg\")\n",
    "        self.neg_files = os.listdir(neg_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pos_files) + len(self.neg_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.pos_files):\n",
    "            file = self.pos_files[idx]\n",
    "            with open(os.path.join(self.root_dir, \"pos\", file), 'r') as f:\n",
    "                text = f.read()\n",
    "            label = 1\n",
    "        else:\n",
    "            file = self.neg_files[idx - len(self.pos_files)]\n",
    "            with open(os.path.join(self.root_dir, \"neg\", file), 'r') as f:\n",
    "                text = f.read()\n",
    "            label = 0\n",
    "        return {\"text\": text, \"label\": label}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    def __init__(self, max_length):\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        labels = []\n",
    "\n",
    "        for item in batch:\n",
    "            text, label = item[\"text\"], item[\"label\"]\n",
    "            indices = [CLS_TOKEN] + [int(b) for b in bytes(text, encoding=\"utf-8\")]\n",
    "            length = min(len(indices), self.max_length)\n",
    "            padding_size = self.max_length - length\n",
    "            indices = indices[:length] + [PAD_TOKEN] * padding_size\n",
    "            attention_mask = [1.] * length + [0.] * padding_size\n",
    "\n",
    "            input_ids.append(indices)\n",
    "            attention_masks.append(attention_mask)\n",
    "            labels.append(label)\n",
    "\n",
    "        return {\"input_ids\": torch.tensor(input_ids), \"attention_mask\": torch.tensor(attention_masks), \"labels\": torch.tensor(labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TextClassificationDataset(\"../datasets/aclImdb\", split=\"train\")\n",
    "test_set = TextClassificationDataset(\"../datasets/aclImdb\", split=\"test\")\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0,\n",
    "                'collate_fn': Collator(MAX_LEN),\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0,\n",
    "                'collate_fn': Collator(MAX_LEN),\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(train_set, **train_params)\n",
    "testing_loader = DataLoader(test_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20000 * 32 / len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(params =  model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for ind, item in enumerate(training_loader):\n",
    "        input_ids, attention_mask, labels = item[\"input_ids\"].to(device), item[\"attention_mask\"].to(device), item[\"labels\"].to(device)\n",
    "\n",
    "        loss = F.binary_cross_entropy(model(input_ids, attention_mask), labels.reshape(-1, 1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if ind % 500 == 0:\n",
    "            with open(LOG_FILE_NAME, \"a\") as f:\n",
    "                print(f\"TRAIN - {epoch=}, loss={loss.item()}\", file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    with torch.no_grad():\n",
    "        for ind, item in enumerate(testing_loader):\n",
    "            input_ids, attention_mask, labels = item[\"input_ids\"].to(device), item[\"attention_mask\"].to(device), item[\"labels\"].to(device)\n",
    "            output = model(input_ids, attention_mask)\n",
    "\n",
    "            loss = F.binary_cross_entropy(output, labels.reshape(-1, 1).float())\n",
    "            losses.append(loss.item())\n",
    "            accurcy = ((output > 0.5) == labels.reshape(-1, 1)).float().mean()\n",
    "            accuracies.append(accurcy.item())\n",
    "\n",
    "    mean_loss = np.mean(losses)\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "\n",
    "    with open(LOG_FILE_NAME, \"a\") as f:\n",
    "        print(f\"EVAL - {epoch=}, {mean_loss=}, {mean_accuracy=}\", file=f)\n",
    "    return np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pablo/long-transformers/playground_transformer.ipynb Cell 19\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Baida-g4/home/pablo/long-transformers/playground_transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m best_test_loss \u001b[39m=\u001b[39m \u001b[39m1e10\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Baida-g4/home/pablo/long-transformers/playground_transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Baida-g4/home/pablo/long-transformers/playground_transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     train(epoch)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Baida-g4/home/pablo/long-transformers/playground_transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Baida-g4/home/pablo/long-transformers/playground_transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         test_loss \u001b[39m=\u001b[39m test(epoch)\n",
      "\u001b[1;32m/home/pablo/long-transformers/playground_transformer.ipynb Cell 19\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Baida-g4/home/pablo/long-transformers/playground_transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Baida-g4/home/pablo/long-transformers/playground_transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m ind, item \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(training_loader):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Baida-g4/home/pablo/long-transformers/playground_transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     input_ids, attention_mask, labels \u001b[39m=\u001b[39m item[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device), item[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device), item[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Baida-g4/home/pablo/long-transformers/playground_transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mbinary_cross_entropy(model(input_ids, attention_mask), labels\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Baida-g4/home/pablo/long-transformers/playground_transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_test_loss = 1e10\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)\n",
    "    if epoch % 5 == 0:\n",
    "        test_loss = test(epoch)\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            torch.save(model.model, MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
