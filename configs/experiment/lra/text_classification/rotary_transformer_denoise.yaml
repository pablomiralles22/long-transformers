# @package _global_
defaults:
  - lra/text_classification/base
  - /model: rotary_transformer
  - /task:
    - denoise

experiment_name: denoise

model:
  layer:
    params:
      d_model: 256
      nhead: 4
      dim_feedforward: 512
      dropout: 0.0
      attention_params:
        type: rotary
        freq: 10000
        disable_half: true
  num_layers: 8

optimizer:
  lr: 1.0e-3
  weight_decay: 0.1

dataset:
  batch_size: 16

task:
  denoise:
    num_embeddings: 259
    mask_token_id: 2

trainer:
  devices: [ 0 ]

callback:
  stochastic_weight_averaging:
    swa_lrs: 1.0e-2