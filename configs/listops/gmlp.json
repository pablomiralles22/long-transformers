{
    "model_params": {
        "name": "layered",
        "params": {
            "input_embedding_dim": 128,
            "output_embedding_dim": 128,
            "layers": [
              [ "gmlp", { "d_model": 128, "hidden_dim": 128, "seq_len": 2000, "dropout": 0.0, "mode": "conv", "activation": "gelu", "selection": "gelu", "num_channels": 32 } ],
              [ "gmlp", { "d_model": 128, "hidden_dim": 128, "seq_len": 2000, "dropout": 0.0, "mode": "conv", "activation": "gelu", "selection": "gelu", "num_channels": 32 } ],
              [ "gmlp", { "d_model": 128, "hidden_dim": 128, "seq_len": 2000, "dropout": 0.0, "mode": "conv", "activation": "gelu", "selection": "gelu", "num_channels": 32 } ],
              [ "gmlp", { "d_model": 128, "hidden_dim": 128, "seq_len": 2000, "dropout": 0.0, "mode": "conv", "activation": "gelu", "selection": "gelu", "num_channels": 32 } ],
              [ "gmlp", { "d_model": 128, "hidden_dim": 128, "seq_len": 2000, "dropout": 0.0, "mode": "conv", "activation": "gelu", "selection": "gelu", "num_channels": 32 } ],
              [ "gmlp", { "d_model": 128, "hidden_dim": 128, "seq_len": 2000, "dropout": 0.0, "mode": "conv", "activation": "gelu", "selection": "gelu", "num_channels": 32 } ],
              [ "gmlp", { "d_model": 128, "hidden_dim": 128, "seq_len": 2000, "dropout": 0.0, "mode": "conv", "activation": "gelu", "selection": "gelu", "num_channels": 32 } ],
              [ "gmlp", { "d_model": 128, "hidden_dim": 128, "seq_len": 2000, "dropout": 0.0, "mode": "conv", "activation": "gelu", "selection": "gelu", "num_channels": 32 } ]
            ]
        }
    },
  "head_params": {
    "ff_dim": 128,
    "num_hidden_layers": 0,
    "dropout_p": 0.0,
    "reduction_method": "mean"
  },
  "data_module_params": {
    "name": "listops",
    "params": {
      "data_path": "datasets/lra_release/listops-1000",
      "max_len": 2001,
      "augment": 1.0,
      "batch_size": 32,
      "pin_memory": true,
      "num_workers": 4
    }
  },
  "optimizer_params": {
    "lr": 1e-3,
    "betas": [ 0.9, 0.98 ],
    "weight_decay": 0.01
  },
  "trainer_params": {
    "max_epochs": 20,
    "val_check_interval": 0.25,
    "precision": "bf16-mixed",
    "enable_checkpointing": true,
    "default_root_dir": "out/listops/gmlp",
    "accelerator": "gpu",
    "devices": [ 2 ]
  },
  "fit_params": {
    "ckpt_path": "out/listops/gmlp/lightning_logs/version_97/checkpoints/epoch=0-val_acc=0.36.ckpt"
  }
}
