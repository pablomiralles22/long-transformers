wandb:
  project: lra-paper
  group: ''
  job_type: training
  mode: online
  save_dir: null
  id: null
  entity: ''
  name: ''
trainer:
  accumulate_grad_batches: 1
  max_epochs: 200
  gradient_clip_val: 0.0
  gradient_clip_algorithm: norm
  log_every_n_steps: 50
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  progress_bar_refresh_rate: 1
  track_grad_norm: -1
  accelerator: gpu
  devices:
  - 0
  default_root_dir: out/${dataset._name_}/${model._name_}/${experiment_name}/${now:%Y-%m-%d}_${now:%H-%M-%S-%f}
task:
  classification:
    _name_: classification
    num_classes: 10
    head_params:
      ff_dim: 0
      num_hidden_layers: 0
      dropout_p: 0.0
      reduction_method: mean
dataset:
  _name_: cifar10
  data_path: false
  batch_size: 32
  max_len: 1024
  augment: true
  pin_memory: true
  num_workers: 6
optimizer:
  _name_: adamw
  lr: 0.001
  weight_decay: 0.0
  betas:
  - 0.9
  - 0.999
scheduler:
  _name_: plateau
  factor: 0.2
  patience: 10
  min_lr: 1.0e-07
experiment_name: base
model:
  _name_: rotary_transformer
  type: single_layer
  layer:
    type: transformer_encoder_layer
    params:
      d_model: 128
      nhead: 4
      dim_feedforward: 256
      dropout: 0.0
      attention_params:
        type: rotary
        freq: 10000
  num_layers: 4
  input_embedding_dim: ${.layer.params.d_model}
  output_embedding_dim: ${.layer.params.d_model}
  params:
    d_model: 160
    nhead: 4
    dim_feedforward: 320
    dropout: 0.0
    attention_params:
      type: rotary
      freq: 10000
